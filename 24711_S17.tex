\documentclass[12pt]{article}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[margin=0.75in]{geometry}
\allowdisplaybreaks

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\usepackage[T1]{fontenc}
\usepackage{ccfonts}
\setlength\parindent{0pt}

\begin{document}

\begin{center}
\textbf{24.711: Topics in Philosophical Logic} \\
Alexander H. Patel \\
{\tt alexanderpatel@college.harvard.edu} \\
Last Updated: \today
\end{center}

These are lecture notes for the Spring 2017 offering of MIT 24.711: Topics in
Philosophical Logic. Pardon any mistakes or typos.

\tableofcontents

\section{February 15, 2017}

\subsection{Zeno's Paradox of the Arrow}

\begin{itemize}
    \itemsep0em 
    \item  An object is at rest during an interval iff it occupies only a
        single position during that interval (not during a moment of time).
        It's stationary during an instant only if the instant is contained in
        an interval during which it's at rest. 
    \item Ancient Greeks could only answer the question qualitatively because
        they couldn't measure time (although they could measure distances). Use
        a compass to make circles and form multiples of a unit length on a
        line.
    \item Can use a straight edge and compass to get an estimate of length to
        any precision up to a rational number, but no further.
    \item Aristotelian physics gives no good way of measuring a length of time.
        Earth, air, wind, fire all have natural motion (up, down), but because
        of acceleration you can't figure out good lengths of time - you need
        something that moves at a uniform speed. The natural motion of things
        in the heavens is circular, so you can use star movement to measure
        lengths of time (not as simple as one would think because Aristotle's
        system was geo-centric where the planets orbited in a circle about the
        earth).
    \item
        Need to be able to measure time and distance in order to measure speed
        and so to get close to a solution for the core question of what it
        means to be stationary.
    \item
        Galileo look at pendulum movement - he saw a smooth continuous movement
        that happens at intervals (PE being converted into KE). The
        Aristotelian saw chaotic movement in the pendulum (you are displacing
        the bob and it's swinging is it trying to go downward but being
        contained by the string). Aristotle didn't have the principle of
        balancing of forces on the bob. Aristotle's answer to the question of
        why the bob doesn't stop is that there is some nonsense with a vacuum
        of air being generated behind the bob and pushing it forward. We can
        measure time by dividing the length of the pendulum rod and noting the
        period is inversely related to the square of the length of the rod.
    \item
        Speed during an interval is the quotient of change of distance over
        change and time. Zeno gave us the idea of 0 speed over an interval, now
        we have a quantitative idea of non-zero speed during an interval. But
        we still have to account for the idea that the ball is at rest only at
        its apogee but at no other point in its path.
    \item 
        We can solve the ball problem by solving $x'(t) = 0$, where $x'$ is the
        limit of the difference quotient of $(x(t + h) - x(t))/h$. Aristotle
        thought that bodies were the only things that were real and that bodies
        aren't made up of points or surfaces. So we can't just think of the
        ball but also need to consider the position in spacetime of the ball -
        we need to think of the trajectory (potential positions occupied by the
        ball) as existent, even when the ball isn't there. But Aristotle argued
        that a line isn't made up of points so this doesn't play out well under
        Aristotelian physics - i.e. we can talk about velocities but not of
        instantaneous velocities.
    \item 
        Why does Aristotle preclude the solution of taking the derivative? Do
        we not just take the limit over smaller and smaller intervals about the
        instant? The Aristotelian post-Newton understood the limit as the
        continual dividing of the line into smaller pieces.
    \item
        Things change when we switch to dynamics - throw a ball up at some
        initial speed and the downward force of gravity will decelerate the
        ball until it reaches 0. But since acceleration is $dv/dt$ there's no
        good way to define instantaneous acceleration out of velocity, which is
        also a function.
    \item
        You know there are times when the ball is going up and times when the
        ball is going down. We want to say there is a time when the ball
        reaches its highest point when its velocity was 0, how do we know there
        was such a time? We could solve this with the IVT but we want to do
        this constructively by constructing the cut between positive velocities
        and negative velocities. Can we get the LUB principle from numbers just
        from the movement of the ball? If the LUB principle didn't hold for the
        numbers then there would points in the ball's flight where it didn't
        have any associated number for its height.
    \item 
        Most of what we can use real numbers for (measurements of things) we
        can get away with just rational numbers. Dedekind had originally the
        bad idea that we could get away with just rational numbers, but that
        was thrown out with $\sqrt{2}$ and the Ancients. Also need cube roots
        to deal with the roots of cubic polynomials. So the real numbers fall
        out of solving polynomials with rational coefficients - but this
        doesn't solve the ball problem because you have no reason to believe
        that the place where the ball is stationary is the root of an equation.
    \item 
        The Ancients don't allow for the idea that there is a point on a line
        that is defined by the all the points to the right of it or to the left
        of it. For the ball, there needs to be a top point to define the switch
        of signs.
    \item
        Is it true that there is a supremum of all the points achieved by the
        ball? Is the ball at rest at this point? What if the ball only achieves
        heights of rational numbers (so it skips between points) or if the ball
        has a hole at the maximum of its height curve?
    \item
        From Mathematica, you have a tire on a hubcap that each go through the
        same number of rotations - how does the hubcap not move much less
        distance than the tire? Galileo: the tire is just a polygon, the hubcap
        goes through places where there is no corner of the polygon
        instantaneously which is just a wacky idea.
    \item 
        Conclusion: you can't explain what is going on if you just think in
        terms of bodies and don't think about space and lines as being made up
        of points.
    \item
        An alternative view says that only bodies are real, and the bodies can
        be in different places but the places aren't real, just the bodies are
        (and the distance relations they have with each other). Kant disputed
        this view - if God made a single marble hand before anything else, then
        He could've made either a right hand or a left hand (it has to be one
        or the other), but if all we have is bodies with spatial relations then
        there is nothing different between a right hand and a left hand; but
        there is a difference between right hand and left hhand so there has to
        be something to account for the difference - you need position.
\end{itemize}

\section{Tuesday, February 21, 2017}

\subsection{Dedekind's Construction of $\mathbb{R}$}

\begin{itemize}
    \itemsep0em 
    \item  
        Jill starts at the bottom of a hill when Jack starts and the top and
        the reach the top/bottom at the same time. Is there a point where they
        are at the same elevation? Yes - take the set of all times when the
        difference in height was positive and then take the least upper bound.
        Most people say that it comes from their motion being continuous.
    \item
        Really early on people learned how to use numbers to measure things -
        at least from having to re-establish property lines after the flooding
        of the Nile. This entailed measurement by yardstick which always yield
        results in fractional form. But can we get these same numerical
        properties without actually deferring to the use of numbers? 
    \item
        The rationals as fractions seem very well understood - the gaps between
        them were less well understood. We know that all the rationals along
        the path will be reached by the ball - how can we justify whether there
        are non-rational points that are hit by the ball.
    \item
        Pythagoreans found that simple ratios of where you put your finger on
        the lyre string correspond to harmonic sounds, but then one of them
        discovered that there are length sthat aren't rationals. He
        didn't find it out by measurement but rather by reductio ad absurdum -
        finding the length of the diagonal of the square with side length 1. He
        showed that it can't be rational (and was thrown overboard for his
        discovery). So you have to have square roots if you want to do geometry
        like the Greeks (because of conic sections).
    \item
        There's a problem which is for any square find another square that has
        twice the area - it was known by Plato and was in the Meno - solve it
        by taking the diagonal of the first as the side length of the second.
        Can you do this with a cube? You can't do this with a compass and
        straight edge. The medeival Arabics pushed forward by going passed
        compass/straightedge and finding teh roots of a cubic equation without
        requiring that the length we're talking about can be one you can
        actually create with a compass and straightedge. That produced a whole
        new family of lengths never talked about by the Greeks. It was widely
        believed that you could create new numbers by constructing equations
        with irrational roots with only rational coefficients. For example $x^3
        = 2$ you would introduce $\sqrt[3]{2}$. It was widely thought that all
        the real numbers were got by solving such equations, but for example
        $\pi$ isn't.
    \item
        Cantor showed that there are many more real numbers that aren't
        solutions to equations than ones there are. The question at the end of
        the 19th century was how to get a picture of what the real line was
        like - how to fill in all the holes. Dedekind's answer was to introduce
        the cuts. To fill in all the gaps, we need a cut everywhere. He gave a
        criteria for what it would mean for $\mathbb{R}$ to be complete, and
        second to show that there really was such a thing as that complete
        system of real numbers. The algebraic properties of the real numbers
        are relatively easy to satisfy, and you need added properties in order
        to fill in all the gaps. The principle Dedekind proposed was the
        least-upper bound principle. He also wanted to convince people that
        this was final - that you couldn't go any further.
    \item
        Dedekind started by seeing if a geometric approach would suffice - he
        thought this was obvious - why? You also need a lot of (controversial)
        set theory to get Dedekind cuts, but the Dedekind cuts do reduce the
        reals to the rationals to the integers. Around this time, everyone
        considered the laws of geometry to be synthetic a priori (Kant), where
        we could only make sense of our sensory experience if we organized them
        into three-dimensional space. It seems dubious that the way we organize
        space should be the foundation of analysis because we have tons of
        applications of the real numbers that have nothing to do with space.
        Also, there's non-Euclidean geometry.
    \item
        Dedekind started by enumerating the rules needed for $\mathbb{R}$:
        field axioms, ordering, etc. But that doesn't distinguish from the
        rationals. You need something more: the completeness principle (LUB).
        Dedekind gives some axioms that the thought could answer the question
        of what the real numbers are, or at least he says the real numbers
        satisfy those structural features. Why is identifying the structural
        features enough to say that you've identified the real numbers? There
        are going to be lots of other algebraic systems that satisfy those
        axioms. You have to also give the intended model for those axioms. The
        physical line satisfies those axioms, but we have no reason to believe
        that any particular one is not the real number system. Also any
        complete model of those axioms is going to be isomorphic to any other.
        Why is giving the structure not enough?
    \item
        At the same time, Frege was investigating the foundations of
        arithmetic. He thought that the only principle he needed was Hume's
        Principle (two collections have the same elements iff there's a
        one-to-one correspondence between the elements). He showed you could
        get PA axioms from Hume's Principle, but he still asked: how do I know
        whether Julius Ceasar is a number? We expect a numeral to pick out a
        definite thing, but we can never do this with only axioms. Dedekind
        introduced the idea that the numerals don't have to stand for definite
        objects - that mathematics is rather about identifying structural
        properties, and that identifying the structure is all you can and need
        to do. 
\end{itemize}

\section{February 22, 2017}

\subsection{Dedekind's Construction of $\mathbb{R}$}

\begin{itemize}
    \item
        Dedekind asked: what further properties than the normal ordering and
        field properties are necessary to specify the real numbers?
        Completeness. His proposal is to create cuts of the rationals into two
        non-empty classes so that everything in the lower portion is less than
        everything in the upper portion. Completeness means that anytime you
        have such a cut there is a real number defining it. This principle
        amounts to the least-upper bound principle. He showed that this
        condition characterizes the real number system (up to isomorphism).
        Historically, people had made systems that purported to be the real
        numbers but it turned out that they left some numbers out. But Dedekind
        showed that you can't add anything to his system and get anything
        that's properly larger. 
    \item
        The isomorphism theorem says that if you have any two models of his
        axioms $a$ and $b$, you first define the positive integers of $a$ to be
        those that contain $0 \in a$ and are closed under succession (and
        define the positive integers of $b$ the same way), and then define a
        map between the two sets of natural numbers. This map is injective - if
        it weren't, then the set of integers that had mappings would have a LUB
        $b'$, and somewhere between $b'$ and $b' - 1$ you would have an integer
        that gets mapped. This lets you map the rationals of $a$ to the
        rationals of $b$, and so then we get an isomorphism of Dedekind cuts
        and so of the real numbers in $a$ and $b$. 
    \item
        So we've completely characterized the structure we're looking for - how
        do we know that there is something that exhibits that structure?
        Dedekind's response is to define the set of all cuts to be the real
        numbers and then show that this structure satisfies the axioms. So he
        takes the construction to be a sufficient answer to the question of
        what the real numbers are. But it feels like he's just given us
        something that is isomorphic to the real numbers. 
    \item
        Frege equivalently gives structural axioms for the natural numbers, but
        wasn't satisfied because he couldn't answer the question of whether
        Julius Caesar was a natural number. Hume's Principle is a structural
        property that only characterizes the natural numbers up to isomorphism.
    \item
        All we need to construct the natural numbers is an infinite set from
        which we can generate a simply infinite set, but that's non-trivial
        because, starting from Zeno's Paradox, there is a long line of people
        saying that we can't have an infinite set, just a potentially infinite
        set. They say that all we can do is give methods for taking a finite
        sequence and extending it, but the idea that we can treat an infinite
        set as a whole was long treated as contradictory. Dedekind thinks he
        can build an infinite set by taking the totality of objects of his
        thought and take a member of that totality S and form the thought "S is
        an object of one's thought", and then recursively do that. That's a
        one-to-one function from the objects of one's thought to the objects of
        one's though, but it isn't onto because one's ego isn't of the form "X
        is the object of my thought". This skirts the question of potentially
        vs. actually infinite because the totality of objects of thought is
        something given to you, not something constructed.
    \item 
        He says that the natural numbers are a "free created of the human mind"
        which he clarifies as being because they are a result of the process of
        abstraction specified above. This isn't completely new - Cantor does
        something similar when he wants to go beyond the natural numbers into
        the transfinite by looking at the different ways of ordering the
        collection and then defining an order isomorphism and then defining the
        order types as got from the orderings by the process of abstraction.
        You regard two orderings the same if their orderings are isomorphic
        (there's a bijection from one to the other that preserves the order).
        Or you can go even more abstract and say that two totalities are the
        same if there is a bijection between them, irrespective of ordering.
        That gives you the cardinal numbers. Aristotle also gets the objects of
        geometry similarly - he thought of them as ordinary solid bodies but we
        abstract away all the non-geometrical properties. Both Cantor and
        Dedekind were dipping into that tradition.
    \item
        In retrospect, it seems like this is a big transition from thinking of
        mathematical terms like "natural/real/complex number" as denoting
        definite things (not things known from sense-experience, but still
        definite entities) to thinking about them as if we can talk about them
        as definite things once we've characterized them up to isomorphism.
        Modern mathematicians are pretty much invariably subscribers to the
        latter view - you talk about mathematical objects, but mathematical
        objects are what they are because of their properties and properties
        only characterize up to isomorphism. There was a change in the way
        mathematicians talked about mathematical objects that philosophy was
        really slow to notice.
    \item
        Cantor also had a way of creating the real numbers out of the
        rationals. Cantor himself was interested in Fourier Series and when
        they converge, for him the idea that the real numbers were complete was
        cashed out by the idea that series converge whenever it's possible for
        them to converge. For any convergent sequence, we have $\forall k,
        \exists N$ such that $n > N \Rightarrow |a_n - a_N| < \frac{1}{k}$
        (Cauchy sequence). So one criterion for the completeness of the real
        numbers other than the LUB principle is the Cauchy criterion implies
        convergence for any sequence. You can derive one criterion from the
        other. Cantor then defined the equivalence relation between $\{x_n\}$
        and $\{y_n\}$ as for any $k$ there is a sequence term such that $y_n$
        and $x_n$ are arbitrarily close passed the sequence term. You can show
        that Dedekind's and Cantor's principles yield isomorphic copies of the
        real numbers. But the question never arose as to which of these is the
        genuine construction of the real numbers. It didn't occur to them to
        ask: what's a real number - a Dedekind cut or an equivalence class of
        Cauchy sequences?
    \item
        Hilbert's axiomitization of geometry: he gave the set of axioms and
        wasn't initially concerned with the question of whether the system was
        complete. We know we get a model of the geometry axioms if you take a
        point to be an ordered triple of real numbers (identify a point with
        its coordinate in $\mathbb{R}^3$, which doesn't have anything to do
        with space and yet is still a perfectly good model). You also get a
        perfectly good model if you identify a point with an ordered triple of
        algebraic numbers - so the geometric axioms do not require
        completeness. Later on, Hilbert started to worry about completeness and
        so he added an axiom in the 9th edition so that the new axiom system is
        complete. One of the axioms of the original system was the Archimedean
        axiom: you can construct a line segment out of a sub-segment so that
        there is some multiple of the sub-segment that will surpass the end of
        the larger segment. Traditionally, geometric axioms were supposed to
        represent three-dimensional space, but Hilbert rather didn't deny that
        the axioms describe physical space but didn't assume that they were
        about physical space. He said the axioms were about systems that
        satisfy those axioms, and nothing more - there's nothing more to being
        an intended model for the geometric axioms. This is similar to group
        theory - there's no intended model for group theory other than a system
        that satisfies those axioms. This is a real departure from the idea
        that mathematics has a definite subject matter.
    \item
        You can have a name without a referent - for example, "Cerberus" or
        other names that occur within myths. You don't have that with names
        like "Cherry" - there's a definite referent to the name "Cherry". But
        we have these numerals in mathematics (e.g. $\sqrt{2}$) that don't have
        a uniquely determined thing that they refer to. The issue with idea of
        identifying the square root of 2 with the set of all things that when
        squared is 2 is problematic because you can't square a set. And you
        can't identify it with any one object whose square is two for similar
        reasons. There's a worry that Dedekind gave us a construction of what
        he called the real numbers and long before him the Egyptians were using
        numerals that purported to be real numbers, so did the Ancient
        Egyptians refer to a Dedekind cut when they wrote down one such
        numeral? Or, possibly, could the real number signs not refer to
        entities but instead to some action, like the act of taking the
        successor with the natural numbers? Well, the numerals do correspond in
        an objective way similarly to how people's names objectively refer to
        people. If it were purely made up, then there wouldn't be a "correct"
        answer for question with real-numbered answers.
    \item
        Dedekind described what he was doing as the reduction of analysis to
        arithmetic, which was understood to have secure foundations. But
        analysis has less secure foundations because people didn't have a clear
        understanding of continuity and limits and what was required for a
        sequence to converge - there was just a general lack of clarity. It's
        more accurate to say that he reduced analysis to set theory or set
        theory plus arithmetic because what he needs to form the cuts is a lot
        of set theory. Looking back, it seems that he really did do something
        important for our understanding of the foundations of analysis, but it
        was not reducing analysis to arithmetic but rather constructing
        analysis within this theory of sets. Next time let's look at the
        axiomatic theory of sets and consider Dedekind's construction as a
        construction of the real numbers within set theory.

\end{itemize}

\section{March 1, 2017}

\subsection{Combinatorial Conception of Sets}

\begin{itemize}
    \itemsep0em 
    \item
        Last time we talked about Zermelo's axiom of set theory. One thing
        Zermelo wasn't addressing but were in the background were the
        paradoxes, which is why people responded to his proof of the
        well-ordering theorem with such uncertainty (their understanding of how
        sets work was shaken by the appearance of paradoxes). Formal set theory
        was still pretty new and their study only came to the foreground with
        Dedekind and Cantor and it seemed to fall immediately into
        contradiction.
    \item
        The most prominent of paradoxes was Russell's paradox - some sets are
        elements of themselves, so consider the set of sets that are not
        members of themselves. If you ask the question of whether that set is a
        member of itself, you get a contradiction. The paradox has a direct
        analogue about language - "dog" isn't a dog so it doesn't apply to
        itself, but the phrase "does not apply to itself" applies to itself iff
        it doesn't.
    \item
        Another paradox was with Cantor's study of ordinal numbers, he found
        that the ordinals were well ordered and for any well ordered set there
        is an ordinal that describes the order type of that set. Any ordinal is
        the order type of the ordinals less than it. But consider the
        collection of all ordinals, which is well ordered, and so there is a
        map from all the ordinals to all the ordinals less than some ordinal.
        Keep applying the map and then you get an infinite descending sequence
        which is a contradiction because the ordinals are supposedly well
        ordered. The semantic analogue is to take all the names of ordinals,
        there are only countably many names and so there are only countable
        many names of ordinals, but since there are uncountably many ordinals
        there are ordinals that aren't named. Since the ordinals are
        well-ordered there is some least ordinal is not named, but we have just
        specified a name for it. Also there is a least natural number nameable
        in fewer than 19 syllables. 
    \item
        Cantor showed that there are uncountably many real numbers by assuming
        that there are only countably many real numbers and then disproving the
        bijection by constructing a new real number not in the enumerated set.
        You can make a bijection $\mathbb{R} \rightarrow \{q_1, q_2, q_3,
        \ldots \in \mathbb{Q}\}$. He generalized the result to show that for
        any set there are more subsets of $S$ than the number of elements of
        $S$. But then take the set of all sets, the cardinal number of the
        power set of the set is larger than the cardinal number of the set
        itself. But the power set of all sets of sets is included in the set of
        all sets and so the cardinal number of the power set is less than or
        equal to the cardinal number of the set of all sets. There is no direct
        linguistic analogue but it came from the generalization of Cantor's
        real number uncountability proof. If we look at that theorem
        linguistically, we see that there are only countably many names for the
        uncountable $\mathbb{R}$, and then put the names in a list an use
        Cantor's procedure to construct a real number that is nameable but not
        on the list of all possible names.
    \item
        Cantor just rejected that there was such thing as the set of all sets.
        He thought there were theological implications. He wrote to the Pope
        (no response) and said that he was developing a theory of the actual
        infinite but there may be worries because the church teaches that only
        God is infinite. He says it isn't blasphemous because he says he has
        found an intermediate level between the finite and the absolutely
        infinite (transfinite) where the transfinite goes beyond what we
        ordinarely think of the finite, but it's not copmlete limitless. If you
        had something completely limitless then you can't number it or
        comprehend it mathematically - only God is at the level of the
        absolutely infinite which is above not just the finite but also the
        transfinite. So Cantor's work isn't bringing God down to the level of
        ordinary mathematicians, but rather he boosted God up to a level
        farther beyond the level of things that Earthly things are. 
    \item
        So there's these analogues between set theoretic paradoxes and semantic
        paradoxes. Zermelo didn't seem interested in the semantic part. Russell
        had the exact opposite attitude - he thought this showed a deep
        discontinuity in human's thought, one that was so profound because it
        manifested in a lot of different ways. How important is it to get a
        unified solution to the set of two paradoxes? It really seems like they
        are the same phenomenon. 
    \item
        This week we're going to look at a pardox that wasn't among the list of
        Russell's paradoxes - developed around 1960. You can't recursively pick
        sets as elements of other sets - there's no infinite descending chain.
        Say a set is well-founded if there aren't any infinite descending
        chains beginning with. Let $W$ be the set of all well-founded sets. If
        there were an infinite descending chain ending with $W$ then look at
        the chain the ends at the element which is an element of $W$ and so
        that chain is infinite and the element is not well-founded. So $W$ has
        to be well-founded. But then $W \in W$ and so there's an infinite chain
        ending at $W$ by picking $W$ for every link of the chain. Since no
        well-founded set is a member of itself, this paradox seems to reduce
        straight to Russell's Paradox. But it doesn't because you could have $A
        \in B \in A \in B \in \dots$. So no loops of any length around whereas
        Russell's paradox is just a loop of length one. This paradox turned
        people's attention to sets that are well-founded - but most sets that
        are useful for math are those that are well-founded. If you want to
        really think closely about set theory but your interest in set theory
        is just for finding a theory of sets that is useful for mathematics,
        then you can just stick with the well-founded sets.
    \item
        So there was an axiom proposed that said that all sets are
        well-founded. What amounts to the same thing is: for any non-empty set
        $S$, there is an element $s \in S$ such that $\forall t \in s, t \notin
        S$. Originally it was just adopted for convenience but it does yield a
        really pretty picture for what the universe of sets looks like. If we
        assume that the things that aren't sets form a set, then you get this
        picture of the theory of types. The crucial axiom for building out this
        structure is the axiom of ordered pairs - this allows you to put
        together two elements from different levels. This is the way that
        Zermelo was thinking about it but not the way that Russell and
        Whitehead were thinking about it. Cantor's paradox entails forming a
        set at some particular level, but the set of all sets must be at a
        level before which the set of all sets is formed and so there is no
        level where the set of all sets is at. Since a set is always formed at
        a level after the level at which its elements are formed, so you solve
        the same set of paradoxes. For the ordinals, each level of the
        hierarchy introduces a new ordinal. Each level of the hierarchy
        introduces exactly one new ordinal. So in order to get the collection
        of all the ordinals it would have to be formed at a stage after which
        you'd already gotten all the ordinals and so there is no collection of
        all the ordinals. So there's a response to all the set-theoretic
        paradoxes that comes just from trying to form a set that violates the
        foundation axiom. 
    \item
        So adding another axiom gave us the stronger system because it removes
        an inconsistency but does not introduce one. If you restrict the axioms
        of set theory to range only over well-founded sets. You can verify that
        each formula you get from an axiom of set theory by restricting the
        quantifier is a theorem of set theory. If you could derive a
        contradiction from the axioms of set theory including the axiom of
        foundation then you could get a contradiction in ZFC.
    \item
        The tradition, beginning with Cantor, is thinking about collections
        formed from contained elements and you need collections to be
        well-founded and so you get paradoxes if you don't restrict to
        well-founded collections. The opposite view is from Frege where you get
        sets by starting with concepts. Von Neumann thought that you get
        contradictions when you start forming sets that are really large ($>>>
        \mathbb{R}$) like the set of all the ordinals or the set of all the
        cardinals, etc. So forming sets that are way too big is impossible but
        formulating sets that are relatively small is okay. The range of a
        function on a set will be at most the same size as the domain set so
        then if the things in the domain aren't too many to form a set then the
        things in the range aren't too many to form a set, either. VN had the
        replacement principle and this more global principle that you can form
        sets unless they are too big, "too big" meaning things form a set
        unless those things are equinumerous with the entire universe. So if
        you have set that's soo big that if it's a set then the entire universe
        is a set then that set is not actually a set. So there's a different
        way of thinking about where the paradoxes come from by having sets that
        are just too big. Axel took this idea and build a whole set theory out
        of it. 
    \item
        This is the combinatorial conception of sets as collections. Next time
        we will think about sets from a logical point of view by starting with
        concepts and then get to sets by a process of abstraction.
\end{itemize}

\section{March 6, 2017}

\subsection{Logical Conception of Sets}

\begin{itemize}
    \itemsep0em 
    \item
        We've been talking about set theory, of which there are two
        conceptions. The combinatorial approach says that sets are collections
        built up from individual. The other idea is the logical conception of
        sets, where you gets sets by starting with concepts and forming sets by
        abstraction. This conception starts with Frege. Frege started out by
        looking at complex names like "the capital of France" and saw that the
        name was formed by "France" and the function sign "the capital of \_".
        "France" names something, but "the capital of" only becomes complete by
        begin supplanted by a name. Concepts are unsaturated and are not
        individuals because it needs another object to be completed. 
    \item
        We know what an incomplete building - there are still bricks and
        mortar, but just not enough to yield a whole. But what is it for a
        concept to be essentially incomplete? At the syntax level, you put a
        name into a concept-phrase; at the semantic level, you put an object
        into a concept to get a truth value. Someone might (incorrectly) think
        that Versailles was the capital of France and say "We're going to Paris
        and then going to the capital of France". But you can't substitute
        "Paris" for the latter reference because the person believes that the
        referrent of "the capital of France" is Versailles. Senses deal with
        things like this (i.e. belief contexts). Thinking about sentences as
        having the True/the False as their referrents turned out to be very
        fruitful because he could think as predicates as functions. This
        allowed standard operations with functions from basic algebra to be
        applied to concepts.
    \item
        Frege has individuals, functions, function signs with more than one
        argument, and second-level functions. Second-level functions: "someone
        is a wise philosopher" has a similar grammatical structure as "Socrates
        is a wise philosopher", so it looks like you have "\_ is a wise
        philosopher" as a concept which can take "Socrates" and "someone" as
        arguments. But in the latter case the individual about which you're
        talking is only indefinitely specified. Frege saw that this wouldn't
        work because of things like "Someone is wise" and "Someone is a
        philosopher" does not yield "Someone is a wise philosopher" and so the
        logical structure of the two is really quite different. Frege's idea is
        that "someone" is not a name, but rather a function sign that takes as
        its argument a concept. So "someone" is the property that a property
        has if there is at least one person who falls under ther latter. So
        things like quantifiers or the definite integral sign are second-order.
    \item
        Frege wanted to use this logical framework to establish arithmetic on a
        purely logical foundation. The prevailing view was that the laws of
        arithmetic were synthetic a priori, where you become acquainted by
        arithmetic by experiencing the act of succession. He wanted to reject
        that view and show that the arithmetic truths are analytic. The
        principle he uses to establish this is called Hume's Principle, which
        says that two concepts have the same number just in case there is a
        one-to-one correspondence between the things that fall under them. He
        wanted to avoid postulating the existence of the numbers, but he has to
        go deeper than Hume's Principle in order to avoid this. He wants to be
        able to think of the laws of number as purely logical laws.
    \item
        Something deeper going on was the theory of classes. The motivating
        idea is that if you say Traveler falls into the class of horses, you
        aren't saying anything more or less than just that Traveler is a
        horse. So he wanted to do something like identify 5 with the class of
        all 5-element sets. But he was able to define the numbers so as to
        avoid the circularity.
    \item
        He had a theory of arithmetic based on counting as derivative from
        Hume's Principle, and then he set out on similar project with the real
        numbers. But then he got a letter from Russell, who pointed out
        Russell's Paradox that comes out of the set of sets that aren't members
        of themselves. Frege (basically) gave up the project, but Russell took
        it over. Frege had gotten to this point by saying that for any concept
        there is a corresponding extension. The idea that you could match
        concepts and objects in a one-to-one fashion is exactly what Cantor
        showed you couldn't do with the real numbers. This is a very general
        argument that there are more concepts than extensions. Russel saw that
        and thought that the way to get around this is not to treat numbers as
        objects but rather as second-level concepts. So there is a concept that
        is true of all concepts under which exactly 5 things fall. The former
        concept is what Russell is going to treat as the number 5. Frege got
        into all this trouble by postulating classes, Russell tried to get away
        without classes. Russell wanted to postulate concepts, and he thought
        that you could guarantee the existence of concepts corresponding to the
        predicates of the language on purely logical grounds. Because from
        "Traveler is a horse" we can infer "$\exists x$ $x$ is a horse" we can
        also get "$\exists F$ Traveler is $F$". He thought that would give you
        on purely logical grounds the existence of concepts.
    \item
        But you can still formulate Russell's Paradox by thinking about the
        concept of not falling under itself. Russell wanted to use concepts in
        all the places where Frege used classes, and even though you can still
        get arithmetic from both you can also still formulate the paradox in
        both. He wanted some principle to ensure that the expressions of the
        language are meaningful because he knew that you could take expressions
        that seemed reasonably and get contradictory results. He appealed to
        Poincare's Vicious Circle Principle, because then you can exclude
        notions like a concept applying to itself. Russell's idea was that you
        need to follow Poincare in order to avoid the paradoxes. He added to
        Frege's levels of concepts (the \textbf{type} of the concept, or the
        number of arguments) the \textbf{order} of the concept which pertains
        to which types of concepts are necessarily for formulating that
        concept. Later, he would replace concepts with propositional functions,
        which are functions that take an object as an argument and gives a
        proposition as its result.
    \item
        The Vicious Circle Principle gets in Russell's way. First, we want to
        be able to state the LUB principle of the real numbers. In terms of the
        vicious circle principle, you have this collection $S$ that is bounded
        above, so you have a collection of upper bounds of $S$ and the least
        member of that collection is the LUB of $S$, which violates the vicious
        circle principle. There's another example of violating the VCP with
        trying to define the natural numbers. To get $\mathbb{N}$, he needs to
        postulate the existence of an infinite set of individuals. Zermelo's
        theory was neutral in that it allowed individuals but didn't require
        individuals. Russell had to require the existence of infinitely many
        individuals, which already was a problem because General Relatively
        says that the universe is finite and QM says that at the level of the
        very small we can get indivisible pieces, so it was actually a question
        of whether there are infinite individuals (at least, the postulate is
        contingent and dubious). If you suppose there is a simply infinite set,
        then you show this by showing a one-to-one function whose range is a
        proper set of the domain and then generating a simply infinite set by
        taking something that is in the range but not in the domain and then
        following that cycle. Russell tried to use the axiom of infinity to get
        0 and the successor operation from this simply infinite set. The
        natural numbers can't just be a set with 0 and everything accessible
        with successor -- you need to be talking about the \textit{smallest}
        collection that containes 0 and closed under successor or else you
        don't get induction. But what is meant by "smallest"? The intersection
        of all collections with 0 and closed under successor. But then you are
        defining the natural numbers as the intersection of all collections
        that contain the natural numbers and your definition is circular. 
\end{itemize}

\section{March 8, 2017}

\subsection{Iterating on the Theory of Types}

\begin{itemize}
    \itemsep0em 
    \item 
        Last time, we ended with a discussion on how Russell's theory of types
        is inadequate for doing basic mathematics (you can't get the real or
        rational numbers).The problem was that the viciuos circle principle was
        too restrictive and issues arose with only being able to define
        propositional functions on lower-order/type functions. As a patch,
        Whitehead and Russell proposed that for any propositional function
        there is a coextensive propositional function with the same type and
        lowest possible order (without talking about anything that isn't in the
        extension of the propositional function). This gave them the
        mathematics they wanted but completely gave up the possibility of
        mathematics being pure logic. As long as all the propositions you
        needed could come just from existential instantiation, then it seems
        that you could just use pure logic. But existential instantiation does
        not get you lower-order correlates of the propositional function.
        Assuming the axiom of reduciability, they really gave up the idea of
        reducing mathematics to logic (and admitted as much).
    \item
       It's pretty similar with the axiom of choice, that if you have a set of
       non-empty overlapping set there's another set that picks one element
       from each of them. To Vann, it seems very obvious if you're thinking
       about classes as collections (you can always form the product space and
       find a projection). If you're thinking not in terms of classes but in
       terms of propositional functions and classes as things that we construct
       by constructing predicates, then there's not really intuitive grounds
       for axiom of choice. If you have infinitely pairs of socks, then there's
       no reason you should be able to form a predicate of only one from each
       pair (??). If you think of sets as being created by some form of mental
       process, then the axiom of choice seems dubious.
    \item
        The Banach-Tarski paradox says that you can cut up a sphere of diameter
        1 into pieces and re-assemble the pieces into two spheres of length 1.
        You need the axiom of choice in order to create the cut. Some people
        thin kit's a problem and other's don't. Why it seems like a problem is
        that you get 8x as much matter for free (it seems). Others say that it
        just seems like a problem because we expect volumes to be preserved,
        but if you think of how volume is defined (you get volume of a sphere
        by inscribing polyhedra inside and outside the square and taking the
        limit of many faces), but you can never really close the gap between
        the inside/outside and the sphere. 
    \item
        Russell had two motives for adopting the vicious circle principle: he
        wanted to respond to the set-theoretic paradoxes and he watned to
        respond to the semantic paradoxes. In the introduction to
        \textit{Principia Mathematica} there is some talk about the semantic
        paradoxes. Ramsey proposed that we distinguish between the semantic
        (epistemic) paradoxes from the set-theoretic paradoxes. For the
        purposes of creating a foundation of mathematics you just focus on the
        set-theoretic paradoxes. Then you realize that you don't need the
        vicious circle principle to solve the set-theoretic paradoxes when you
        have the axiom of reducibility. Ramsey suggested that you scrap order
        altogether and just arrange the propositional functions so that a
        function just has to be a higher type than its arguments. Quine took
        that a step further: Frege wanted to talk about classes, Russell wanted
        to replace that with talk of propositional functions. Quine noticed
        that the talk of propositional functions was intended for the logical
        reduction project, but since that failed we might as well go back to
        talking about classes. So we had individuals, classes of individuals,
        relations (binary/ternary/so on) between individuals, binary relations
        relating classes to classes, individuals to classes, etc. So you end up
        with this structure that is still kid of complicated but still much
        simpler than Russell's solution.
    \item
        Norbert Weiner saw that even that picture could be simplified by
        instead of talking about relations, let's identify a relation with a
        class of ordered pairs/triples/etc. (identify $<a, b, c>$ with $<a, <b,
        c>>$. So you can just talk about individuals, classes of individuals,
        classes of classes of individuals, etc. Weiner's idea was that you
        didn't have to treat ordered pairs as primitive. The law of ordered
        pairs says that $<a, b> = <c, d>$ iff $a = c$ and $b = d$. Norbert
        noticed that you could take $\{\{\{a\}, \emptyset\} , \{\{b\}\}\}$, and
        you can verify that this satisfies the law of rodered pairs. There are
        more brackets than you'd think because he was working in the theory of
        types and need to be working with things of the same type. With this
        simplication, what started as a monstrously complicated system becomes
        very simple.
    \item
        To get mathematics, he needed infinitely many individuals. If you have
        only finitely many individuals, no matter how high up the hierarchy he
        went you'd still only have finitely many things. But if the individuals
        about which you're talking are physical objects than it is actually
        pretty dubious that are infinitely many. So they added the Axiom of
        Infinity and they didn't regard it as obviously true.
    \item
        There's still one feature of that project that is still kind of
        anomalous. You want to say that there are 8 people in the room, and how
        we're going to make sense of that is by saying that the number 8 is the
        collection of all 8-element classes of individuals. So a number is a
        class of classes of individuals such that every class of individuals
        that can be put in one-to-one correspondence with a member of the class
        is a member of the class. What is still peculiar about this theory of
        types is that you can't mix types which means you can't use the same
        numbers to count individuals as to count classes of individuals. So
        there's a different number system of each type and there are infinitely
        many systems of natural numbers. It's not fatal, but it's just a little
        fussy and seems kind of artificial. 
    \item
        G\"odel stepped in a suggested that there is no good reason not to make
        the classes cumulative instead of only allowing members of type $n - 1$
        in classes of type $n$. Originally, we wanted this hierarchy of classes
        in order to protect us from the paradoxes and the vicious circle
        principle, but we've given up the vicious circle principle. He noticed
        that we get the paradoxes when we ask whether a member of the class is
        at the same type as the class, and so we can lighten the restriction to
        allowing any type below the class, not just the one type just below it.
        Originally, we get the type hierarchy from a grammatical restriction on
        concepts/objects, but once you allow cumulative classes you need to
        abandon the grammatical distinction - so you say that you won't make
        type distinctions among the things we're talking about as far as logic
        is concerned.  Some of the things we will talk about are individuals,
        sets of individuals, and so forth, but we're going to be able to talk
        about those all at once without distinguishing between them (there's
        only one kind of variable for the logic). What gets us the hierarchical
        structure is not a grammatical restrictuion but rather an axiom that
        tells us that the sets are well-founded. so in G\"odel's system you
        have a very simple logic (first-order predicate calculus) but you have
        a lot of not simple axioms guaranteeing the existence of sets. 
    \item
        So it looks as if there is no hope for reducing mathematics to logic,
        it looks like Cantor and Dedekind won. The advantages of a logical
        conception of set over a combinatorial conception of set disappear once
        this project doesn't work. And ZF set theory is so much more powerful
        because it can extend into the transfinite. But this is also possibly
        bad because it can make it more vulnerable to contradiction. Still
        there are lot more points that have gone to the combinatorial
        conception rather than the logical conception. There is still a lot to
        be desired and it's a big loss to have mathematics be synthetic a
        priori rather than analytic. 
    \item
        G\"odel's incompleteness theorem throws a wrench in the works by
        showing that there are arithmetic truths that we can recognize as true
        that aren't derivable from the axioms, and that even if you add more
        axioms there are still more truths that aren't provable.
    \item
        More recently, the Neologicist program has been trying not to get us
        all of ZFC but rather to get us mathematics strong enough in order to
        serve or purposes will staying in an (arguably) logical framework.
    \item
        Dedekind proved that arithmetic is categorical, but if you think of
        arithmetic as a first-order theory the way we do know, then that is not
        categorical (it is not a second-order theory). You get induction from a
        quantification over properties and you get categoricity from induction.
        But when you do things in a first-order framework you have an axiom
        schema for induction and so you have infinitely many induction axioms
        instead of a single second-order axiom. But then this new set of axioms
        isn't categorical and by G\"odel's work isn't complete. 
    \item
        The next part of our story is going to be a revival of second-order
        logic as something we can do in teh context of modern mathematics when
        we're well beyond the \textit{Principia} stage. That revival is due to
        George Boolos. Next time, we'll talk about Boolos and Quine and then
        we'll start talking about Benacerraf. Read Quine and Boolos articles.

\end{itemize}

\section{March 15, 2017}

\subsection{Quine's Nominalism}

\begin{itemize}
    \itemsep0em 
    \item 
        Quine really wants to be a thorough-going nominalist. He wants to deny
        that there are any abstract objects. But he doesn't think he can do it
        for reasons that we'll spend most of April talking about. Right now,
        we're going to talk about his more-limited version of nominalism. 
    \item
        He's willing, albeit reluctantly, to concede that there are such things
        as classes, but he wants to deny the existence of universals and
        properties. His reasons for doing that are mostly having to do with the
        absucrity of the identity conditions for properties.
    \item
        You can tell whether two classes are the same, which is in the case
        where they have all the same elements. It is hard to tell whether two
        properties are the same. It's interesting that Russell went the other
        way, trying to get rid of classes and use only properties.
    \item
        It doesn't really matter to us that Quine wants to deny that they are
        properties. What is a more interesting question is how is that even an
        intelligbile thought. We see all the dogs and are able to distinguish
        all the thigns that are dogs from those that are not dogs. How can we
        classify that which is a dog and that which is not? 
    \item
        The obvious answer is that there are some properties that dogs have in
        common and we can use those properties for our classification. Quine
        wants to deny that there are these properties that the dogs have in
        common. 
    \item
        One approach is to take the nominalist view that the only thing dogs
        have in common is the name we give them. But this approach is bad
        because it isn't arbitrary how we classify the dogs but it is arbitrary
        how we name them. This is how Russell thought of it. To say, 'X is a
        dog', 'Y is a dog', and 'Z is not a dog' all logically entail that
        there is a property that X and Y share but Z lacks.
    \item
        There's this position that Quine wants to advocate that there are not
        any properties, but it is hard to formulate that in a way that a
        competent English speaker wouldn't regard as obviously false.
    \item
        The idea is that there is something that X and Y have that Z lacks. The
        thought is that there is this property that is shared between the
        former but not by the later. Question: why can't you not specify what
        they have in common but instead just say that they are both dogs.
    \item
        Or is there a difference between properties and propositional
        functions. Common doctrine is that 'that' clauses denote propositional
        functions. If you use 'that' clauses as direct objects of verbs (??)
        commit us to propositional functions.
    \item
        But Quine wants to deny that there are any propositional functions.
        Didn't we learn anything from going through this pointless complexity
        of going through the ramified theory of types. We really just need an
        inclusive view of what objects there are. In particular, we don't need
        a special logical category for propositional functions. Just put them
        in the same domain as your quantifiers. It's going to make our life
        more complicated without adding much by build distinctions (like type
        distinctions) into the logic. So for type distinctions we just need
        some way to demarcate the differences within the universe of the domain
        of the quanitification instead of adding logical distinctions. 
    \item
        Quine's answer to his own question: first, he has a nice slogan. "To be
        is to be the value of an variable", which tells us a method for
        answering the question of what the ontological commitments are of a
        given theory. That seems to be as far as it goes but really doesn't
        seem to go very far at all because anybody who doesn't agree that
        because X, Y, and Z all walk on four legs and share the property of
        being four-legged. Anyone who wants to deny that is not a competent
        speaker of English. 
    \item
        Quine proposes that this criterion is right but in order to employ it
        you need to enter a process of translating English into a canonical
        form where you are trying to fully make the commitments of your theory
        as explicit as possible. In particular, Quine says that if you want to
        make it explicit what the ontological commitments of your theory are,
        then reformulate the theory within the first-order predicate calculus.
        Then, the things that have to serve in the variable place tell you what
        the ontological commitments of your theory are. He doesn't think this
        is obligatory but he thinks that if people refuse then it means that
        they are refusing to make their own logical/ontological commitments
        explicit.
    \item
        It seems like this is demanding too much. The machinations Carnap had
        to go through in order to talk about disposition terms (??) in the
        first-order predicate calculus. To talk about what it means for a
        chemical to be soluable: you know that water disolves sugar, soy ou can
        see which particular samples of sugar dissolve and you can predict
        which won't. You can predict that sawdust cannot dissolve. So a natural
        formulation is $x$ is soluable iff $x$ dissolves when put into water.
        But if you use this as your material conditional, you'll get that
        things that are never put into water are soluble. But you don't want
        that. Carnap went through this big effort to try and figure out a way
        to express solubility in the first-order predicate calculus by saying:
        if a theory of solubility contains two axioms:
        \begin{enumerate}
            \item Disolves in water
            \item Two identical substances either both disolves or neither of
                them does. (??)
        \end{enumerate}
        But if something is never put into water and nothing else like it is
        ever put into water than you cannot say that is it not soluble.
    \item
        You get something that looks a lot more natural if you're allowed to
        understand the conditional that $x$ is soluble if it dissolves when put
        into water - to understand this is a much more strong material
        conditional (if it were put into water, it would dissolve) - that seems
        like a use of disposition terms is deeply entrenched in our efforts to
        describe the world in science and without. And our use of
        counterfactual conditionals is also a very fruitful usage. It seems
        necessary restrictive to go with Quine and restrict to the
        lower-predicate calculus.
    \item
        But if you accept Quine's criteria you seem to be getting bogus
        ontological commitments inasmuch as you want to understand disposition
        terms in terms of counterfactuals. Well, you can't take counterfactual
        usage as primitive because you have to ultimately formulate things in
        terms of the predicate calculus, but you cant take such usage as
        defined by adopting a possible world semantics. $x$ is soluble if it
        would dissolve in the closest possible world. But we wanted to avoid
        talking about possible worlds because we wanted only to look at what is
        around us and speak about that.
    \item
        Vann wanted to return to the discussion we were having last time about
        the possible skeptical conclusions from the L\"owenheim-Skolem theorem.
        Beyond our theorizing, there is a use of number talk in counting and
        measuring that doesn't really seem to get us significance beyond what
        we get from just the theorems. The fear is that if we accept Quine's
        mandate to restrict ourselves to first-order logic, we seem to be
        forced into some sort of skeptical position. Our mathematical theories
        don't have a unique intended model. That is something that we saw
        already from Dedekind and that's something we feel like we can cope
        with. What Dedekind did was to capture the real and rational numbers in
        a way that was unique up to isomorphism, but didn't go any farther than
        that to specify which of the isomorphic structures was the real number
        system. Look at how Dedekind develops the theory: that his terms
        doesn't have uniquely determined referrents doesn't seem to cause any
        problems for the way he does mathematics, even just for practice;
        classifying up to isomorphims it seems good enough. Take whatever our
        mathematical theory is, assumign infinite models, it is going to have
        wildly disimilar infinite models that don't resemble each other at all
        structurally.
    \item
        Something Putnam wants to insist on is that we don't appeal to occult
        powers of the mind to see how our terms latch on to their determinate
        referrents. But we're not we're just using number talk that leave it
        drastically undetermined what the referrents of our terms is. We get a
        kind of skepticism that is upstream from the kind of skepticism you get
        from thinking about whetehr theorems/axioms are really true.  This is
        at a level of skepticism that beings even earlier. Nevermind how you
        know they're true, how are you going to have mathematical beliefs if
        supposedly beliefs are about these objects but there is nothing that
        you do that is even close to picking out those mathematical objects.
    \item
        Skolem's paradox was from the early paradox and was easily answered
        just by saying that you get the elementary submodel by neglecting some
        functions that we know are really there. So, yeah, if you neglect
        things you can get a theory that looks just like our theory. That looks
        like a sleepy little paradox. But then Putnam took it and made it look
        like a very serious paradox, both inside and outside of mathematics.
    \item
        There's a way to get around the paradox which is to do what Dedekind
        did and say that contemporary mathematics' describe teh natural numbers
        etc. as models of a first-order theory. Dedekind proceeded differently:
        wrote down his axioms as a second-order theory where he could range
        quantifiers over properties and was able to prove categoricity (that
        his axioms actually determined the real or natural numbers uniquely up
        to isomorphism). But his proof took place within this second-order
        theory, Can't we reapply the L\"owenheim-Skolem argument to the higher
        theory. Unless we assume that we have some grasp of what properties
        there are that can't be explained in how we use property talk. So you
        just get back to using occult powers of the mind again. 
    \item
        There's an answer to Skolem's problem that claims to be different than
        just kicking the problem upstairs to higher-order logic. Though whether
        or not it really is different is controversial. The alternative answer
        started with Peter Geach with the project of formulating ordinary
        English sentences into ordinary language. He was particularly
        interested in the use of plural noun phrases. Take the sentence: "there
        are some critics that only admire one another". The sentence "there are
        some critics who just admire impressionists" can be formulated easily
        in a first-order idiom. But "there are some critics that only admire
        one another" can only be expressed in a first-order idiom if we can
        appeal to the use of classes. 
        $$(\exists S)((\exists x) x \in S \centerdot (\forall y)(y \in S
        \Rightarrow \mathrm{Critic}(y)) \forall x \forall y(x \in S \centerdot
        \mathrm{Admire}(x, y) \Rightarrow y \in S))$$
        But then:
        $$(\exists S)(\exists x Sx \centerdot \forall y Sy \Rightarrow
        \mathrm{Critic}(y) \forall x \forall y S x \centerdot
        \mathrm{Admires}(x, y))$$
        But if I say that there are critics who admire each other it doesn't
        seem like I am telling you anything about classes and you could imagine
        someone who is a strict nominalist and certainly believe that there are
        critics, you can think anything you like about their group relations,
        and you can belief that there are critics who admire only one another,
        but you can't be a nominalist and only belief this sentence. If you
        accept the commonly held believes about sentences then you learn that
        there are the same truth conditions for the first and second sentences
        but you may belief the things about critics without believing the
        reformulation.
    \item
        You can't formulate the statement about classes just in terms of
        first-order logic; take the sentence: "there are some critics who only
        admire one another". That sentence has the same form as "there are some
        non-zero natural numbers that are adjacent only to one another" but the
        latter sentence is only going to be true in a non-standard model of
        arithmetic. If you could successfully formulate in first-order logic
        that there are some critics who only admire one another then you could
        successfully formulate that there are some non-zero natural numbers
        that are adjacent to one another, then you could construct a sentence
        that is true in the standard models but not in the non-standard models.
        But then it is not possible to give a categorical description of the
        natural numbers system.
    \item
        Boolos proposed that the correct way to understand that kind of use of
        plurals we see in the last sentence is by taking a different kind of
        the use of the quantifiers. We have quantifiers that range over
        individuals, second-order quantification where they range over
        properties. Boolos is proposing the notion of plural quantification
        where the variables range over individuals and the difference is that
        in singular quantification the variables range over individuals one at
        a time while with plural quantification the quantifiers range over
        individuals many at a time. So there is in the semantics for
        first-order predicate calculus a variable assignment is a function
        assigning a value with each of the variables. Then you inductively
        define what it is for a variable assignment to satisfy an open
        sentence. 
    \item
        There are critics who only admire one another with: $$\exists xx
        ((\forall y)y \in xx \rightarrow (\mathrm{Critic}(y) \centerdot
        (\forall y \forall z) (y \in xx \centerdot \mathrm{Admire}(y, z))
        \Rightarrow z \in xx))$$ Note the $\exists xx$ which is the plural
        quanitifier. This theory doesn't posulate any new things and doesn't
        add any new attributes or properties to get the range. Both quantifiers
        range over individuals. The plural variables are as restrictive because
        you can have multiple individuals instead of just one: you have to
        associate at least one individual but you can associate more than one.
        Once we have plural quantification we get our categorial descriptions
        of the objects of classical mathematics. 
    \item
        The key to dscribing the natural numbers if: for any set of natural
        numbers, there is one among them that is less than all the others. If
        you want to classify the real numbers, you have to know: for any real
        numbers, if there is a number that is greater or equal to all of them
        then there is a least number that is greater or equal to all of them.
        Zermelo formulated the separation principle in terms of definite
        properties, people weren't happy with brining the metaphysics of
        properties into set theory so Z said that we should formulate it istead
        as a first-order theory, with the difficulty that the first-order
        schemata admits non-standard models. Now that we have plural
        quantification, we can say that for any set $X$, $\forall x \in X$,
        $\exists Y$ such that only the $x$ such taht $x \in X$ are members of
        $Y$. W ecan similarly give a plural quantification version with the
        replacement axiom schema. Vann will embarass himself if he tries and
        says it, but there is a second-order formulation of the replacement
        axiom, if you insert the replacements into ZF set theory then you do
        not get a categorical representation but you get instead ZF axioms and
        their plurally quantified versions and the fact that for any two models
        of the axioms either they are isomorphic or one of them is isomorphic
        to an initial segment of the other, which you get by clipping off the
        taller model. So that means we don't get a categorical generalization
        of the universe of set theorem.
    \item
        The continuum problem comes from Cantor's proof that the number of real
        numbers is greater than the number of natural numbers. Are there any
        infinite sets with cardinality between the two? That question is going
        to have a definite answer, especially if we're using plural
        quantification. The axioms of set theory do not have a uniquely
        determined model, but they are just alike to the part of the model that
        is smaller, and the real numbers and all the functions defined on them
        are in the part of the universe below the first inaccessible, and so if
        the continuum hypothesis is true an one model of ZFC reformulated in
        terms of plural quantification, then it is true in all the models and
        so the continuum hypothesis is going to have a definite answer. It
        would take a bit of patience to verify or disprove the continuum
        hypothesis because you have to check equinumerosity with all possible
        subsets of the real numbers, of which there are one or two.
    \item
        We have lots of results about things being undecidable in first-order
        ZFC. The only things we have that are undecidable in second-order ZFC
        will be if it implies some large-cardinal axiom. If you can prove that
        this sentence proves the large-cardinal axiom and you think it's
        consistent (but you can't prove it), then that will tell you that it is
        independent of plural quantification of ZFC. There also aren't that
        many second-order models just like with first-order models. It's so
        hard to construct models that there is hardly anything to say about
        second-order model theory.
\end{itemize}

\section{March 20, 2017}

\subsection{Benacerraf, "What Numbers Could Not Be"}

\begin{itemize}
    \itemsep0em 
    \item 
        Von Neumann and Zermelo had different set-theoretic definitions for the
        natural numbers: $3 = \{2\}$ for one and $3 = \{0,1,2\}$. It seems like
        they are two ways of representing the same thing; but it also seems
        like these are two ways of misrepresenting because you don't really
        have strict identity here.
    \item
        They're following a tradition that begins with Dedekind: you get an
        infinite set if you can it can be put in one-to-one correspondence with
        a proper subset of itself, and then you get a simply infinite set by
        taking something that is not in the image of the one-to-one function
        and then repeatedly applying the one-to-one function to it. Dedekind
        identified a simply infinite set and then said that we can take the
        natural numbers to be any simply infinite set we'd like. But if you
        really do it that way that it seems like each of us can have our own
        choice for what is the natural numbers; how would we be able to
        communicate? 
    \item
        Paul Benacerraf, who gave us this parable, said that each answer works
        equally well and so there is no reason to prefer one over the other. In
        fact, any simply infinite progression will be a satisfactory
        representation of the natural number system (originally with
        recursiveness, later he realized he doesn't need it). 
    \item
        Some definitions may serve better use or be more aesthetically
        pleasing, but this is a question of the referrent of a term and about
        the definitions of the natural numbers.
    \item
        Benacerraf specified that the progression needed to be recursive, but
        here it is a bit unclear what is meant by a set of sets being recursive
        (rather than a set of integers). Something maybe like the method of
        counting to obtain numerals needs to be recursive. It is easy to
        understand the association of numerals with sets of sets and to ask
        which of those types of sets constructions are recursive (there is ID
        between an algorithm an a recursive set of integers). Recall that
        $V^\omega$ is the set of finite sets all of whose members are finite
        sets all of whose members are finite sets, and so on. From that
        perspective, it seems that Benacerraf is requires a much stronger
        condition: that the progression has to consist of elements from
        $V^\omega$. 
    \item
        Once you identify the natural numbers with some simply infinite
        progression, then you can define the operations of addition,
        multiplication, and less than on that infinite progress and then after
        that you just treet members of that infinite progression in exactly the
        same way as everyone learns to treat numbers. The fact that we are
        talking about sets here doesn't make any different for how we do
        arithmetic or counting. What is important is that the series is simply
        infinite, but once you have that then arithmetic doesn't change.
    \item
        We want to reduce number theory to set theory, so we're going to say
        that every numeral denotes a set. The embarassment is that you need the
        numerals to name sets in some such way that you never have two numerals
        denote the same set, but beyond that it looks completely arbitrary
        which sets you choose. Why that is troublesome is that, if the numerals
        are names of numbers, then for a name to be part of a non-fiction story
        it must have a referrent out in the world. So we have these numerals
        that grammatically act like names but have no referrents, then are we
        saying that arithmetic is just a fiction. Benacerraf sees this as
        pushing us towards a skeptical conclusion. the idea is that, for a long
        time, people thought that the Iliad and the Odyssey were entirely
        fictitious and that there was no place as Troy. So if the Iliad is an
        entirely fictitious story, then there wouldn't have been any such city
        as Troy. There is (was) such a city, so the Iliad is not entirely
        fictitious. If there is nothing out there in the world that is referred
        to by "3", then is it fictitious? This situation is similar to color
        words ("red") in that you can use "red" both as an adjective and as a
        noun. When you use "2" as an adjective, you are going to us a plural
        noun; with color, you can also use with a singular noun (but you can't
        with "2").
    \item
        Benacerraf ends the article with a bewildering sentence: "there are no
        such things as numbers; which is not to say that there are not at least
        two prime numbers between 15 and 20". What the heck does that mean?
    \item
        Here's a possibility: does this all come about because people decided
        they wanted to reduce arithmetic to set theory? Zermelo was keen on
        having set theory as the foundation of mathematics (it makes for an
        efficient theory with a clean ontology). Do the problems we've been
        talking about persist even without the set theory?
    \item
        Vann asks: could we have an example of an intrinsic problem? Maybe
        self-identity or being abstract? It really seems right to say that
        Gauss knew what the numbers were, although he certainly did not have
        any reduction of number theory to set theory. Maybe the problem is
        thinking that number theory should be reduced to set theory, maybe the
        problem is to say that number theory can be ontologically reduced to
        set theory (as in, you can identity numbers with sets). Maybe the
        reduction of number theory to set theory went one step too far: the
        really useful thing was to see that the natural numbers are isomorphic
        to the several simply infinite progressions identitified by Von Neumann
        and by Zermelo. That turns out to be really useful because set theory
        has powerful mathematical methods that aren't going to be available to
        you if you restrict your attention to numbers (there are features of
        $\mathbb{N}$ that are only recognizable by the methods of higher
        mathematics). But as an ontological reduction if you say that the only
        things there are are sets, then your mathematics doesn't change - you
        get all the mathematics from just realizing that $\mathbb{N}$ is
        isomophorphic to the simple progressions. The new theorems you could
        prove after making the identification are things like $7 \in 17$. Vann
        is wondering if this is all a tempest in a teapot, where you get these
        difficulties and this controversy because you are insisting that the
        numbers are actually identical with the sets - but there is no benefit
        in that. Just say that numbers are their own kind of thing or throw up
        your hands and not decide whether the numbers are sets or not.
    \item
        There were some really good motives that lay underneath the reduction
        of mathematics to set theory: one is that you need uniform definitions
        that could be understood across the different branches of mathematics
        (there is a lot of crossover in the field). You can't cross over if the
        definition of continuity in complex analysis is at odds with the
        definition of continuity in another field. In the 19th century, there
        were lots of quarrels because there weren't good definitions. There is
        agreement among mathematicians about what constitutes a valid proof,
        but this doesn't require the identification of the natural numbers with
        steps. It maybe leaves you with a slightly bloated ontology because now
        you have both sets and numbers instead of just sets, but the benefits
        of ontological reduction aren't all that great when the reduction isn't
        all that efficient. For example, reducing heat to molecular kinetic
        energy actually explains what heat is, but reducing natural numbers to
        sets doesn't add any explanatory power. 

\end{itemize}

\end{document}

